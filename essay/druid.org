* druid
   - github https://github.com/metamx/druid
  
** Design · metamx/druid Wiki
https://github.com/metamx/druid/wiki/Design

*** Introduction
Druid currently allows for single-table queries in a similar manner to Dremel and PowerDrill. It adds to the mix （功能介于dremel以及powerdrill之间）
   - columnar storage format for partially nested data structures（对于嵌套数据结构使用列式存储）
   - hierarchical query distribution with intermediate pruning（层级查询的时候会进行剪枝）
   - indexing for quick filtering（使用index进行快速过滤）
   - realtime ingestion (ingested data is immediately available for querying)（能够处理实时的数据）
   - fault-tolerant distributed architecture that doesn’t lose data（容错性）
As far as a comparison of systems is concerned, Druid sits in between PowerDrill and Dremel on the spectrum of functionality. It implements almost everything Dremel offers (Dremel handles arbitrary nested data structures while Druid only allows for a single level of array-based nesting) and gets into some of the interesting data layout and compression methods from PowerDrill. （druid基本提供了dremel所提到的功能，但是不允许嵌套数据结构只允许数组，而借鉴了一些powerdrill的数据存储方式和压缩算法） *NOTE(drirlt):druid不支持跨表查询，只是支持单表query，这点和dremel非常类似，而cloudera的impala支持join操作）*

*** Architecture
   - The name comes from the Druid class in many role-playing games: it is a shape-shifter, capable of taking many different forms to fulfill various different roles in a group.(druid名字在许多角色游戏里面都有出现，它能够变形以适应各种不同的角色）

The node types that currently exist are:（有下面几种节点类型存在）
   - Compute nodes are the workhorses that handle storage and querying on “historical” data (non-realtime) 处理历史数据
   - Realtime nodes ingest data in real-time, they are in charge of listening to a stream of incoming data and making it available immediately inside the Druid system. As data they have ingested ages, they hand it off to the compute nodes. 处理实时数据，并且能够将实时数据导入到druid系统里面供compute节点使用 *NOTE（dirlt）：我对realtime这个部分的数据处理还是比较好奇的，因为这个部分的数据似乎不像是使用druid描述那些方法来进行query的*
   - Master nodes act as coordinators. They look over the grouping of computes and make sure that data is available, replicated and in a generally “optimal” configuration. 作为协调者存在，判断节点是否挂掉，确保数据能够available以及replicated.
   - Broker nodes understand the topology of data across all of the other nodes in the cluster and re-write and route queries accordingly 相当于router的角色存在，知道每个节点上面存在哪些数据，接收query请求并且将query请求改写让realtime和compute节点计算，然后做汇总。
   - Indexer nodes form a cluster of workers to load batch and real-time data into the system as well as allow for alterations to the data stored in the system (as of 11/2012, these nodes are still a work in progress) 对历史数据和realtime数据进行索引。
工作方式大致是：
   - 请求发送到broker上面
   - broker会改写这个query statement，并且根据route信息将这些sub query发送到compute/realtime节点进行计算
   - compute/realtime返回结果，breoker将结果进行聚合返回。
*NOTE（dirlt）：似乎是单层的结构，但是不是非常确定*

另外整个系统还需要依赖外部三个组组件：
   - A running ZooKeeper cluster for cluster service discovery and maintenance of current data topology（zookeeper记录topology）
   - A MySQL instance for maintenance of metadata about the data segments that should be served by the system（MySQL存储segment的metadata)
   - A “deep storage” LOB store/file system to hold the stored segments(deep storage用来保存alpha data set以及beta data set)

*** Data Storage
Getting data into the Druid system requires an indexing process. This gives the system a chance to analyze the data, add indexing structures, compress and adjust the layout in an attempt to optimize query speed. (数据进入druid之前会进行一些分析并且进行索引，然后压缩并且调整存储格式来优化查询速度） A quick list of what happens to the data follows.
   - Converted to columnar format
   - Indexed with bitmap indexes
   - Compressed using various algorithms
       - LZF (switching to Snappy is on the roadmap, not yet implemented)
       - Dictionary encoding w/ id storage minimization
       - Bitmap compression
       - RLE (on the roadmap, but not yet implemented)
The output of the indexing process is stored in a “deep storage” LOB store/file system (S3 is the currently implemented solution, HDFS is planned). Data is then loaded by compute nodes by first downloading the data to their local disk and then memory mapping it before serving queries.(输出结果保存在LOB里面，现在是使用S3存储接口今后会支持HDFS。数据的话首先会被download到本地然后进行serve）

In order for a segment to exist inside of the cluster, an entry has to be added to a table in a MySQL instance. This entry is a self-describing bit of metadata about the segment, it includes things like the schema of the segment, the size, and the location on deep storage. These entries are what the Master uses to know what data should be available on the cluster. （segment的metadata会保存在MySQL里面。master会用这些信息来判断哪些信息是没有被serve的） *NOTE（dirlt）：个人觉得实现方式应该类似与，indexer运行不是很频繁，每次indexer对数据进行index的时候会将segment的metadata写入MySQL，完成之后通知Master来读取*

*** Fault Tolerance
   - Compute As discussed above, if a compute node dies, another compute node can take its place and there is no fear of data loss 
   - Master Can be run in a hot fail-over configuration. If no masters are running, then changes to the data topology will stop happening (no new data and no data balancing decisions), but the system will continue to run.（如果master挂掉的话那么不允许进行topology的变化，不允许新增数据以及数据的balance) *NOTE(dirlt):S3的存储难道没有解决balance以及replication的问题？*
   - Broker Can be run in parallel or in hot fail-over.
   - Realtime Depending on the semantics of the delivery stream, multiple of these can be run in parallel processing the exact same stream. They periodically checkpoint to disk and eventually push out to the Computes. Steps are taken to be able to recover from process death, but loss of access to the local disk can result in data loss if this is the only method of adding data to the system. *TODO（dirlt）；realtime介绍不是很多*
   - “deep storage” file system If this is not available, new data will not be able to enter the cluster, but the cluster will continue operating as is.
   - MySQL If this is not available, the master will be unable to find out about new segments in the system, but it will continue with its current view of the segments that should exist in the cluster.（不能够知道新的segment加入）
   - ZooKeeper If this is not available, data topology changes will not be able to be made, but the Brokers will maintain their most recent view of the data topology and continue serving requests accordingly.（如果compute节点挂掉的话那么检测不到。对于路由信息broker本身会保存一份副本）

** Introducing Druid: Real-Time Analytics at a Billion Rows Per Second | Metamarkets
http://metamarkets.com/2011/druid-part-i-real-time-analytics-at-a-billion-rows-per-second/

*** Background
Druid is the distributed, in-memory OLAP data store that resulted.（分布式全内存的OLAP存储系统） 有两个名字需要稍微了解一下：
   - roll up，降维比如使用group
   - drill down. roll up的反义

文中以下面的实际例子来进行分析。假设我们有如下的data table数据集合成为alpha data set,这个部分数据是存放在硬盘上面的。
#+BEGIN_VERSE
timestamp             publisher          advertiser  gender  country  .. dimensions ..   click  price
2011-01-01T01:01:35Z  bieberfever.com    google.com  Male    USA                         0      0.65
2011-01-01T01:03:63Z  bieberfever.com    google.com  Male    USA                         0      0.62
2011-01-01T01:04:51Z  bieberfever.com    google.com  Male    USA                         1      0.45
...
2011-01-01T01:00:00Z  ultratrimfast.com  google.com  Female  UK                          0      0.87
2011-01-01T02:00:00Z  ultratrimfast.com  google.com  Female  UK                          0      0.99
2011-01-01T02:00:00Z  ultratrimfast.com  google.com  Female  UK                          1      1.53

...
#+END_VERSE

为了能够将数据放入内存，在alpha data set上面做roll up形成beta data set.这个部分可以放入内存。roll up方法如下
#+BEGIN_VERSE
    GROUP BY timestamp, publisher, advertiser, gender, country
      :: impressions = COUNT(1),  clicks = SUM(click),  revenue = SUM(price)
#+END_VERSE
产生的beta data set如下：
#+BEGIN_VERSE
timestamp             publisher          advertiser  gender  country  impressions  clicks  revenue
2011-01-01T01:00:00Z  ultratrimfast.com  google.com  Male    USA      1800         25      15.70
2011-01-01T01:00:00Z  bieberfever.com    google.com  Male    USA      2912         42      29.18
2011-01-01T02:00:00Z  ultratrimfast.com  google.com  Male    UK       1953         17      17.31
2011-01-01T02:00:00Z  bieberfever.com    google.com  Male    UK       3194         170     34.01
#+END_VERSE

beta data set主要包含3个部分：
   - Timestamp column: We treat timestamp separately because all of our queries center around the time axis. Timestamps are faceted by varying granularities (hourly, in the example above).（时间列主要用来做时间范围内的查询，roll up时候可以按照不同的粒度切片，上面的例子是按照小时）
   - Dimension columns: Here we have four dimensions of publisher, advertiser, gender, and country. They each represent an axis of the data that we’ve chosen to slice across.（维度列主要用来进行交叉查询）
   - Metric columns: These are impressions, clicks and revenue. These represent values, usually numeric, which are derived from an aggregation operation – such as count, sum, and mean (we also run variance and higher moment calculations). For example, in the first row, the revenue metric of 15.70 is the sum of 1800 event-level prices.（指标列则是一些具体的数值）

对于这些数据集合上面我们可能需要有下面这些操作：
   - “How many impressions from males were on bieberfever.com?” and 
   - “What is the average cost to advertise to women at ultratrimfast.com?”  
   - But we have a hard requirement to meet: we want queries over any arbitrary combination of dimensions at sub-second latencies.
理论上这个集合可能非常大，但是实际上这个大部分的维度交叉item还是非常少的，比如few Kazakhstanis visit beiberfever.com

*** Failed Solution I: Dynamic Roll-Ups with a RDBMS
So about a year ago, we fired up a RDBMS instance (actually, the Greenplum Community Edition, running on an m1.large EC2 box) 开始使用一些关系数据库，但是存在下面这些问题：
   - We stored the data in a star schema, which meant that there was operational overhead maintaining dimension and fact tables. *TOOO（dirlt）：不太明白这个是什么意思*
     - 关于star schema可以参考这篇文章 Salina & IT Mind: Data Warehouse: Star Schema http://salinaitmind.blogspot.jp/2012/10/data-warehouse-star-schema.html
     - 所谓的fact table里面存放的是具体值，而dimensional table是指属性或者说是维度。
     - dimensional table引用的都是fact table里的值。
     - 一个fact table（多个fact table也行）可能会被多个dimensional table所引用，这样就形成了一个星型schema。 
   - Whenever we needed to do a full table scan, for things like global counts, the queries ran slow. For example, naive benchmarks showed scanning 33 million rows took 3 seconds. （对于一些全表扫描的操作非常地慢 *NOTE（dirlt）：这个主要是因为没有使用并行查询的方式把？* ）
     - We started materializing all dimensional roll-ups of a certain depth, and began routing queries to these pre-aggregated tables. We also implemented a caching layer in front of our queries. （开始通过pre roll up到一定的深度然后在这些table上面进行查询，并且在query之前假设一个cache layer）
     - This approach generally worked and is, I believe, a fairly common strategy in the space. Except, when things weren’t in the cache and a query couldn’t be mapped to a pre-aggregated table, we were back to full scans and slow performance.（上面工作方式在大部分时候工作还是很好的，但是如果没有出现在cache或者是pre-compute的table里面性能就非常差）
     - We tried indexing our way out of it, but given that we are allowing arbitrary combinations of dimensions, we couldn’t really take advantage of composite indexes. （尝试建立二级联合索引，但是因为允许在所有的dimenson上面进行查询，所以还是不行）
     - Additionally, index merge strategies are not always implemented, or only implemented for bitmap indexes, depending on the flavor of RDBMS.（另外一些RDBMS的index merge策略可能没有实现，或者只是实现了bitmap index merge策略 *TODO（dirlt）：不太明白什么意思？* ）
We also benchmarked plain Postgres, MySQL, and InfoBright, but did not observe dramatically better performance.

*** Failed Solution II: Pre-compute the World in NoSQL
   - In short, we took all of our data and pre-computed aggregates for every combination of dimensions. At query time we need only locate the specific pre-computed aggregate and and return it: an O(1) key-value lookup. This made things fast and worked wonderfully when we had a six dimension beta data set.(在NoSQL里面需要预先计算很多维度的组合，但是在查询的时候非常快。如果维度只有6个的时候还是工作非常快速的）
   - But when we added five more dimensions – giving us 11 dimensions total – the time to pre-compute all aggregates became unmanageably large (such that we never waited more than 24 hours required to see it finish).（但是我们测试11个维度的时候，发现计算量太大）
   - Lesson learned: massively scalable counter systems like rainbird are intended for high cardinality data sets with pre-defined hierarchical drill-downs. But they break down when supporting arbitrary drill downs across all dimensions. （NoSQL不太适合高维度的查询，只是适合低纬度并且能够预先计算的场景）
     
*** Introducing Druid: A Distributed, In-Memory OLAP Store
下面是前面两种方式各自的问题：
   - Relational Database Architectures
       - Full table scans were slow, regardless of the storage engine used
       - Maintaining proper dimension tables, indexes and aggregate tables was painful
       - Parallelization of queries was not always supported or non-trivial
   - Massive NOSQL With Pre-Computation
       - Supporting high dimensional OLAP requires pre-computing an exponentially large amount of data

Keeping everything in memory provides fast scans, but it does introduce a new problem: machine memory is limited. The corollary thus was: distribute the data over multiple machines. （内存有限的话通过将数据分布在多个机器上面）

Thus, our requirements were:
   - Ability to load up, store, and query data sets in memory （放在内存里面避免了load up时间）
   - Parallelized architecture that allows us to add more machines in order to relieve memory pressure（分布式查询能够减缓memory压力）
And then we threw in a couple more that seemed like good ideas:
   - Parallelized queries to speed up full scan processing （同时分布式查询可以加快full scan处理速度）
   - No dimensional tables to manage （不维护任何dimensional table）

These are the requirements we used to implement Druid. The system makes a number of simplifying assumptions that fit our use case (namely that all analytics are time-based) and integrates access to real-time and historical data for a configurable amount of time into the past.（做了一些假设来简化设计比如所有的分析都是按照时间来进行划分的，并且支持对实时和非实时数据的统一访问）

** Druid, Part Deux: Three Principles for Fast, Distributed OLAP | Metamarkets
http://metamarkets.com/2011/druid-part-deux-three-principles-for-fast-distributed-olap/

*** Partial Aggregates + In-Memory + Indexes => Fast Queries
   - alpha represents the raw, unaggregated event logs, while beta is its partially aggregated derivative. （将alpha dataset使用部分聚合形成beta dataset)
   - The key to Druid’s speed is maintaining the beta data entirely in memory. Full scans are several orders of magnitude faster in memory than via disk. What we lose in having to compute roll-ups on the fly, we make up for with speed.(将beta data set存放在memory里面）
   - To support drill-downs on specific dimensions (such as results for only ‘bieberfever.com’), we maintain a set of inverted indices.(为了支持在beta dataset上面做drill down，需要维护一个反向索引，这个在另外一片文章里面提到了，主要使用bitmap来表示entry在alpha dataset中的位置，并且对应的表示非常容易进行and/or/not)     
  
*** Distributed Data + Parallelizable Queries => Horizontal Scalability
   - Druid’s performance depends on having memory — lots of it. We achieve the requisite memory scale by dynamically distributing data across a cluster of nodes. As the data set grows, we can horizontally expand by adding more machines.(通过动态地在节点中分布数据来达到比较方便的水平扩展）
   - To facilitate rebalancing, we take chunks of beta data and index them into segments based on time ranges.（为了能够完成rebalance，将beta dataset分片并且进行索引，根据时间范围）
   - For high cardinality dimensions, distributing by time isn’t enough (we generally try to keep segments no larger than 20M rows), so we have introduced partitioning. We store metadata about segments within the query layer and partitioning logic within the segment generation code.（而对于维度比较多的内容，仅仅按照时间分布还是不够的，我们尽量让我一个segment不要超过20M rows所以需要引入partition。 *NOTE（dirlt）：这个partition应该是用户自己定义的* 然后druid将segment的metadata保存在qeury layer上面，而用户在查询的时候需要自己提供partition的code）
   - We persist these segments in a storage system (currently S3) that is accessible from all nodes. If a node goes down, Zookeeper coordinates the remaining live nodes to reconstitute the missing beta set.（segment数据也会在S3文件系统上面进行持久化。这样如果一个server node挂掉的，可以选举另外一个节点从S3文件系统中读取beta dataset。检测node挂掉通过zookeeper协调）
   - Downstream clients of the API are insulated from this rebalancing: Druid’s query API seamlessly handles changes in cluster topology.（下游的client则不需要考虑rebalance的情况）
   - Queries against the Druid cluster are perfectly horizontal. We limited the aggregation operations we support – count, mean, variance and other parametric statistics – that are inherently parallelizable. While less parallelizable operations, such as median, are not supported, this limitation is offset by rich support of histogram and higher-order moment stores. The co-location of processing with in-memory data on each node reduces network load and dramatically improves performance.（限制进行聚合的操作，确保这些操作确实可以并行完成。 如果没有并行完成的话，可以通过  *histogram and higher-order moment stores（高阶矩）* 的支持来补偿 *TODO（dirlt）：WTF is that?* 

*** Real-Time Analytics: Immutable Past, Append-Only Future
   - For real-time analytics, we have an event stream that flows into a set of real-time indexers. These are servers that advertise responsibility for the most recent 60 minutes of data and nothing more. (对于实时分析有专门都的real-time indexer server，处理最近60分钟的数据）
   - They aggregate the real-time feed and periodically push an index segment to our storage system. The segment then gets loaded into memory of a standard server, and is flushed from the real-time indexer.（定期将real-time和历史数据做合并然后刷新real-time的数据）
   - Similarly, for long-range historical data that we want to make available, but not keep hot, we have deep-history servers. These use a memory mapping strategy for addressing segments, rather than loading them all into memory. This provides access to long-range data while maintaining the high-performance that our customers expect for near-term data.（对于那些非常老的历史数据，使用deep-history servers工作方式，使用mmap来访问segments而不用完全载入内存）
